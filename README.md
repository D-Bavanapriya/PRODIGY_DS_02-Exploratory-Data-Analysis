# PRODIGY_DS_02-Exploratory Data Analysis
# Task 02
This is my Task 2 of the Data Science Internship at Prodigy Infotech. This repository is focused on performing Exploratory Data Analysis (EDA) on the famous Titanic dataset, which is essential for understanding data patterns, uncovering insights, and preparing data for further modeling. Through visualizations and statistical analysis, the project sheds light on relationships between various features, identifying trends that could help in predicting passenger survival.
# Files
Titanic dataset EDA.ipynb: 
A detailed Jupyter Notebook showcasing the entire EDA process, including data cleaning, visualization, and statistical analysis. The notebook walks through the different steps required to explore and process the Titanic dataset effectively.

test.csv: 
The dataset used for analysis, containing passenger information such as class, gender, age, etc., and whether they survived or not.
# Key Steps in the EDA Process
1. Data Cleaning
Handling Missing Values: Missing data was identified and imputed or removed accordingly.
Outliers Detection: Outliers were detected through visualizations such as box plots.
Correcting Data Types: Numerical and categorical features were verified and corrected for proper analysis.
Feature Engineering: New features were derived from existing data to capture more relevant patterns.
2. Data Visualization
Univariate Analysis: Features such as age, fare, and class were analyzed individually to understand their distributions.
Bivariate & Multivariate Analysis: Correlations between variables such as age, gender, and survival were explored through visualizations such as heatmaps, bar charts, and scatter plots.
Survival Distribution Analysis: Key patterns about passengers who survived vs. those who did not were uncovered using categorical analysis.
Visual Tools Used: Libraries like Matplotlib and Seaborn were used to generate insightful plots.
3. Statistical Insights
Descriptive Statistics: Key statistical metrics like mean, median, mode, and standard deviation were computed to better understand the dataset.
Correlations: Relationships between variables were explored using Pearson correlation coefficients and visualized through heatmaps.
4. Preparing Data for Machine Learning
Feature Encoding: Categorical variables were converted into numerical form for modeling.
Data Splitting: The dataset was prepared for training and validation by splitting it appropriately.
# Technologies Used
Python: Core programming language for the analysis.
Pandas: Used for data manipulation and analysis.
Matplotlib & Seaborn: Libraries for creating visualizations.
Jupyter Notebook: Environment to perform EDA and present the analysis.
# Project Highlights
Data Cleaning: Dealt with missing data and anomalies.
Descriptive Statistics: Provided a statistical overview of the dataset.
Visualization: Created a variety of plots to analyze relationships between features.
Feature Engineering: Derived new features that could enhance prediction models.
